{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VJLgHEXUj6g"
   },
   "source": [
    "##드롭아웃 아담"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69Y1a2FGVAWV"
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "          self.dropout_ratio = dropout_ratio\n",
    "          self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "          if train_flg:\n",
    "              self.mask = np.random. rand(*x.shape) > self.dropout_ratio\n",
    "              return x * self.mask\n",
    "          else:\n",
    "              return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1N13rytipTep"
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update (self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items ():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt (1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        for key in params.keys ():\n",
    "            self.m[key] += (1 - self.betal) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads [key]**2 - self.v[key])\n",
    "\n",
    "            params[key] -= lr_t * self.m[key] / (np.sart(self.v[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQT37Uq9oNKS"
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys:\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saZ0QK9VUnU1"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DeJqqQ4Ufsu"
   },
   "source": [
    "##GoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1Cwg9CHZ6St"
   },
   "source": [
    "https://roytravel.tistory.com/338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiZ5KkpmWXp8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj) -> None:\n",
    "        super(Inception, self).__init__()\n",
    "        self.branch1 = ConvBlock(in_channels, n1x1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, n3x3_reduce, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(n3x3_reduce, n3x3, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, n5x5_reduce, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(n5x5_reduce, n5x5, kernel_size=5, stride=1, padding=2))\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels, pool_proj, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)\n",
    "\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, aux_logits=True, num_classes=1000) -> None:\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        assert aux_logits == True or aux_logits == False\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = ConvBlock(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = ConvBlock(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)\n",
    "        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            self.aux2 = None\n",
    "\n",
    "    def transform_input(self, x: Tensor) -> Tensor:\n",
    "        x_R = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "        x_G = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "        x_B = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "        x = torch.cat([x_R, x_G, x_B], 1)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.transform_input(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.a4(x)\n",
    "        aux1: Optional[Tensor] = None\n",
    "        if self.aux_logits and self.training:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.b4(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.d4(x)\n",
    "        aux2: Optional[Tensor] = None\n",
    "        if self.aux_logits and self.training:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.e4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1) # x = x.reshape(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.aux_logits and self.training:\n",
    "            return aux1, aux2\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs) -> None:\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes) -> None:\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv = ConvBlock(in_channels, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(3, 3, 224, 224)\n",
    "    model = GoogLeNet(aux_logits=True, num_classes=1000)\n",
    "    print (model(x)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN1ZLRpWUrfe"
   },
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QB5DoJ7UrPl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, AvgPool2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKEQzdhUVAaJ",
    "outputId": "df4aabdb-2e24-4d72-b61c-176db4986398"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet_5(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_5,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "model = LeNet_5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icCRBt9WZTsA",
    "outputId": "5046333b-2b55-4613-ac0d-99bceca0c8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2db7wGqpZVVY",
    "outputId": "054be9ea-c961-4a1e-b592-da16b5d35704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "            Conv2d-2           [-1, 16, 10, 10]           2,416\n",
      "            Conv2d-3            [-1, 120, 1, 1]          48,120\n",
      "            Linear-4                   [-1, 84]          10,164\n",
      "            Linear-5                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_uInQLJZYSI"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQN3OW7KZZ0i"
   },
   "outputs": [],
   "source": [
    "# optimizer 정의합니다.\n",
    "from torch import optim\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 현재 lr을 계산하는 함수를 정의합니다.\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "# 러닝레이트 스케쥴러를 정의합니다.\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "lr_scheduler = CosineAnnealingLR(opt, T_max=2, eta_min=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvnjaMUbZdHI"
   },
   "outputs": [],
   "source": [
    "# 배치당 performance metric 을 계산하는 함수 정의\n",
    "def metrics_batch(output, target):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2wZHQcdZfGC"
   },
   "outputs": [],
   "source": [
    "# 배치당 loss를 계산하는 함수를 정의\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target)\n",
    "    metric_b = metrics_batch(output, target)\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return loss.item(), metric_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqYMfE5wZgqH"
   },
   "outputs": [],
   "source": [
    "# epoch당 loss와 performance metric을 계산하는 함수 정의\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.type(torch.float).to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "        running_loss += loss_b\n",
    "\n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "        \n",
    "        if sanity_check is True: # sanity_check가 True이면 1epoch만 학습합니다.\n",
    "            break\n",
    "\n",
    "    loss = running_loss / float(len_data)\n",
    "    metric = running_metric / float(len_data)\n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0DSyITiZlGZ"
   },
   "outputs": [],
   "source": [
    "# train_val 함수 정의\n",
    "def train_val(model, params):\n",
    "    num_epochs = params['num_epochs']\n",
    "    loss_func = params['loss_func']\n",
    "    opt = params['optimizer']\n",
    "    train_dl = params['train_dl']\n",
    "    val_dl = params['val_dl']\n",
    "    sanity_check = params['sanity_check']\n",
    "    lr_scheduler = params['lr_scheduler']\n",
    "    path2weights = params['path2weights']\n",
    "\n",
    "    loss_history = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "    }\n",
    "\n",
    "    metric_history = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "    }\n",
    "\n",
    "    # best model parameter를 저장합니다.\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs-1, current_lr))\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "            loss_history['val'].append(val_loss)\n",
    "            metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights')\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print('train loss: %.6f, dev loss: %.6f, accuracy: %.2f' %(train_loss, val_loss, 100*val_metric))\n",
    "        print('-'*10)\n",
    "\n",
    "    # best model을 반환합니다.\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeH2SFsEZn73"
   },
   "outputs": [],
   "source": [
    "# 모델을 학습합니다.\n",
    "model,loss_hist,metric_hist=train_val(model,params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdeDhUB_XYJ5"
   },
   "source": [
    "##VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC-zckypVdWW"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #3 224 128\n",
    "            nn.Conv2d(3, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #64 112 64\n",
    "            nn.Conv2d(64, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #128 56 32\n",
    "            nn.Conv2d(128, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #256 28 16\n",
    "            nn.Conv2d(256, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #512 14 8\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        #512 7 4\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(7)\n",
    "        #512 1 1\n",
    "        self.classifier = nn.Linear(512, 1000)\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(512*2*2,4096)\n",
    "        self.fc2 = nn.Linear(4096,4096)\n",
    "        self.fc3 = nn.Linear(4096,10)\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        features = self.conv(x)\n",
    "        #print(features.size())\n",
    "        x = self.avg_pool(features)\n",
    "        #print(avg_pool.size())\n",
    "        x = x.view(features.size(0), -1)\n",
    "        #print(flatten.size())\n",
    "        x = self.classifier(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUOkeIh1Ya1V"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "param = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmPHiW1nY4Gj"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.00001)\n",
    "\n",
    "idx2label = []\n",
    "cls2label = {}\n",
    "with open(\"./imagenet_class_index.json\", \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb5v-p8vZA3J"
   },
   "outputs": [],
   "source": [
    "or epoch in range(3):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    if(epoch>0):\n",
    "        net = Net()\n",
    "        net.load_state_dict(torch.load(save_path))\n",
    "        net.to(device)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs,f = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(loss.item() > 1000):\n",
    "            print(loss.item())\n",
    "            for param in net.parameters():\n",
    "                print(param.data)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    save_path=\"/data/data1/minjoo/workspace/vgg/imagenet_vgg16_result.pth\"\n",
    "    torch.save(net.state_dict(), save_path)\n",
    "        #print(\"\\n\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "class_correct = list(0. for i in range(1000))\n",
    "class_total = list(0. for i in range(1000))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs,_ = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(16):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "accuracy_sum=0\n",
    "for i in range(1000):\n",
    "    temp = 100 * class_correct[i] / class_total[i]\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        idx2label[i], temp))\n",
    "    accuracy_sum+=temp\n",
    "print('Accuracy average: ', accuracy_sum/1000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
